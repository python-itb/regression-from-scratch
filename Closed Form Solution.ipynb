{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression in a nutshell is a very direct approach to predict response Y based on X. It assumes that there are linear relationship between X and Y. Mathematically, this is defined as\n",
    "\n",
    "$$Y \\approx b_0 + b_1X $$\n",
    "\n",
    "Here, $b_0$ corresponds to intercept while $b_1$ corresponds to slope/gradient. Now, our goal is to estimate $b_0$ and $b_1$ so the plot looks 'nice'. \n",
    "\n",
    "<img src=\"./images/RSS.png\" width=40%>\n",
    "\n",
    "In mathematical notation, this is commonly referred as RSS. \n",
    "\n",
    "RSS is simply the sum of squared difference between target $y$ and estimated value $\\hat{y}$\n",
    " \n",
    "$$\\sum(y-\\hat{y})^2$$\n",
    "\n",
    "Remember that $\\hat{y}$ is equal to $b_0+b_1x$ so we can expand this further into\n",
    "$$\\sum(y-(b_0+b_1x))^2$$\n",
    "\n",
    "Our goal is to find the 'correct' $b_0$ and $b_1$ so that RSS is minimized. Meanwhile, the plot of RSS vs $b_0$ and $b_1$ might look like this\n",
    "\n",
    "<img src=\"./images/RSS_vs_param.png\" width=40%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The great thing about this RSS function is that it always have a global minimum. This is confirmed by calculus by setting the derivation of the RSS function to zero and directly find the $b_0$ and $b_1$. However, this value could correspond either to global maximum or minimum, we don't know for sure. After we have computed the second derivative we would have confirmed that the RSS function is a convex function, which means that it must have a unique, global minimum. However, RSS is a special case where the partial derivation can be set to zero so we could obtain the 'correct' parameter. More often than not, we could not do this. So take this closed form solution with a little bit grain of salt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would derive RSS with respect to (w.r.t.) $b_0$ and $b_1$ before we set them both to 0 and find the parameter $b_0$ and $b_1$\n",
    "\n",
    "Let's now call RSS function as a cost function or a loss function. In a nutshell, its the function we strive to maximize or minimize. And in this specific case, we want to minimize the difference the RSS. In other words, what we're looking for is the $b_0$ and $b_1$ that would minimize the RSS. Intuitively, we would always find the minimum of cost function, but it turns out that in some cases, i.e logistic regression we would find the maximum value of the cost function. It's a little bit off topic, but a little bit of anticipation is always nice.\n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    " \\frac{\\partial C}{\\partial b_0} &=& -2 \\sum(y-b_0-b_1x)\\nonumber\\\\\n",
    "   &=& -2 \\sum(y-(b_0+b_1x))\\nonumber\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    " \\frac{\\partial C}{\\partial b_1} &=& -2 \\sum(y-b_0-b_1x)(x)\\nonumber\\\\\n",
    "   &=& -2 \\sum(y-(b_0+b_1x))(x)\\nonumber\\\\\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the first equation to zero\n",
    "\n",
    "\\begin{eqnarray}\n",
    " 0 &=& -2 \\sum(y-b_0-b_1x)\\nonumber\\\\\n",
    "   &=& \\sum(y) - \\sum(b_0) - \\sum(b_1)x\\nonumber\\\\\n",
    " b_0 &=& \\frac{y}{n} - \\frac {b1 \\sum(x)}{n}\\nonumber\\\\\n",
    " &=& \\overline{y} - b_1\\overline{x}\\nonumber\\\\\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the second equation to zero\n",
    "\n",
    "\\begin{eqnarray}\n",
    " 0 &=& -2 \\sum y-b_0-b_1xx\\nonumber\\\\\n",
    "   &=& \\sum yx - \\sum b_0x - \\sum b_1x^2\\nonumber\\\\\n",
    " b_1\\sum x^2 &=&  \\sum yx - b_0\\sum x\\nonumber\\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "But recall that $b_0 = \\overline{y} - b_1\\overline{x}$\n",
    "\n",
    "\\begin{eqnarray}\n",
    " b_1\\sum(x^2) &=&  \\sum(yx) - (\\overline{y} - b_1\\overline{x})\\sum(x)\\nonumber\\\\\n",
    "   &=& \\sum yx - (\\frac {\\sum y}{n} - b_1 \\frac{\\sum x}{n})\\sum x \\nonumber\\\\\n",
    "   &=& \\sum yx - \\frac{1}{n} \\sum y \\sum x  + \\frac{1}{n} b1 \\sum x \\sum x\\nonumber\\\\\n",
    " b_1 (\\sum x^2 - \\frac {1}{n} \\sum x \\sum x)  &=& \\sum yx - \\frac{1}{n} \\sum y \\sum x\\nonumber\\\\\n",
    " b_1 &=& \\frac {\\sum xy - \\frac{1}{n} \\sum x \\sum y} {\\sum x^2 - \\frac {1}{n} \\sum x \\sum x}\\nonumber\\\\\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, most of the times, we are not able to do this, since the cost function is much more complicated. That is why we would use Gradient Descent most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
